{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from model import VideoSaliencyModel\n",
    "import argparse\n",
    "from utils import *\n",
    "from os.path import join\n",
    "from torchvision import transforms\n",
    "import yaml\n",
    "from PIL import Image\n",
    "\n",
    "import concurrent.futures\n",
    "import concurrent.futures\n",
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "from moviepy import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "VIDEO_DIR = '/ssd_scratch/cvit/sarthak395/outputs/IyMdcXl4vag/raw_videos'\n",
    "OUTPUT_DIR = '/ssd_scratch/cvit/sarthak395/outputs/IyMdcXl4vag/saliency_results'\n",
    "BATCH_SIZE = 16\n",
    "FILE_WEIGHT = '/home2/sarthak395/Sony_Shorts_Creator/ViNet_Saliency/saved_models/ViNet_DHF1K.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyPredictionDataset(Dataset):\n",
    "    def __init__(self , video_dir , save_dir , len_temporal = 32):\n",
    "        self.video_dir = video_dir\n",
    "        self.len_temporal = len_temporal\n",
    "        self.save_dir = save_dir\n",
    "        self.video_list = os.listdir(video_dir) # contains the video names : ['IyMdcXl4vag_1.mp4' , 'IyMdcXl4vag_2.mp4' , ...]\n",
    "        self.video_list.sort()\n",
    "        self._create_samples()\n",
    "    \n",
    "    def _create_samples(self):\n",
    "        self.samples = [] # contains the samples of the form (video_path , start_frame , end_frame , save_path , flip)\n",
    "        for video_name in self.video_list:\n",
    "            video_path = os.path.join(self.video_dir , video_name)\n",
    "            # clip = VideoFileClip(video_path)# remove logs\n",
    "            clip  = VideoFileClip(video_path)\n",
    "            num_frames = int(clip.fps * clip.duration)\n",
    "            \n",
    "            # processing the initial (len_temporal-1) frames\n",
    "            for i in range(self.len_temporal - 1):\n",
    "                if i < num_frames:\n",
    "                    start_frame = i\n",
    "                    end_frame = i + self.len_temporal - 1 # total frames = len_temporal\n",
    "                    save_path = os.path.join(self.save_dir , video_name.split('.')[0] , f'frame_{i:04d}.png')\n",
    "                    self.samples.append((video_path , start_frame , end_frame , save_path , True)) # need to flip the frames\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # processing the rest of the frames\n",
    "            for i in range(self.len_temporal - 1 , num_frames):\n",
    "                start_frame = i - self.len_temporal + 1\n",
    "                end_frame = i\n",
    "                save_path = os.path.join(self.save_dir , video_name.split('.')[0] , f'frame_{i:04d}.png')\n",
    "                self.samples.append((video_path , start_frame , end_frame , save_path , False))\n",
    "            \n",
    "        print(f'Total samples created : {len(self.samples)}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path , start_frame , end_frame , save_path , flip = self.samples[idx]\n",
    "        clip = VideoFileClip(video_path)\n",
    "        \n",
    "        # can you use moviepy to read the frames from start_frame to end_frame\n",
    "        assert end_frame - start_frame + 1 == self.len_temporal , f'len(frames) = {len(frames)} != {self.len_temporal}'\n",
    "        num_frames = int(clip.fps * clip.duration)\n",
    "        subclip = clip.subclipped(start_frame / clip.fps , (end_frame+1) / clip.fps)\n",
    "\n",
    "        frames = list(subclip.iter_frames(fps=clip.fps, dtype='uint8'))\n",
    "        frames = [Image.fromarray(frame).convert('RGB') for frame in frames]\n",
    "        frames = [self.torch_transform(frame)[0] for frame in frames]\n",
    "\n",
    "        # make frames to be of length len_temporal , first by selecting only first len_temporal frames\n",
    "        # and then by padding the rest with zeros\n",
    "        frames = frames[:self.len_temporal]\n",
    "        if len(frames) < self.len_temporal:\n",
    "            for i in range(self.len_temporal - len(frames)):\n",
    "                if not flip:\n",
    "                    frames.insert(0 , torch.zeros_like(frames[0]))\n",
    "                else:\n",
    "                    frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "        clip = torch.FloatTensor(torch.stack(frames, dim=0)) # (len_temporal , 3 , H , W)\n",
    "        # clip = clip.permute((0,2,1,3,4)) # \n",
    "        clip = clip.permute((1,0,2,3)) # (3 , len_temporal , H , W)\n",
    "        if flip:\n",
    "            clip = torch.flip(clip , [1])\n",
    "\n",
    "        return clip , idx\n",
    "    \n",
    "    def torch_transform(self , img):\n",
    "        img_transform = transforms.Compose([\n",
    "                transforms.Resize((224, 384)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.485, 0.456, 0.406],\n",
    "                    [0.229, 0.224, 0.225]\n",
    "                )\n",
    "        ])\n",
    "        sz = img.size\n",
    "        img = img_transform(img)\n",
    "        return img, sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_prediction_dataset = SaliencyPredictionDataset(VIDEO_DIR , OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = saliency_prediction_dataset[2030]\n",
    "print(\"sample shape : \" , sample[0].shape)\n",
    "print(\"sample save path : \" , saliency_prediction_dataset.samples[sample[1]][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_prediction_dataloader = DataLoader(saliency_prediction_dataset , batch_size = BATCH_SIZE , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out a sample from the dataloader\n",
    "sample = next(iter(saliency_prediction_dataloader))\n",
    "print(sample[0].shape) # (BATCH_SIZE , 3 , len_temporal , H , W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoSaliencyModel(\n",
    "    transformer_in_channel=32, \n",
    "    nhead=4,\n",
    "    use_upsample=True,\n",
    "    num_hier=3,\n",
    "    num_clips=32   \n",
    ")\n",
    "model.load_state_dict(torch.load(FILE_WEIGHT))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET A PREDICTION FOR A SINGLE SAMPLE\n",
    "test_sample = next(iter(saliency_prediction_dataloader))\n",
    "test_output = model(test_sample[0].to(device))\n",
    "print(test_output.shape) # (BATCH_SIZE , 1 , H , W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [854, 480], 'bitrate': 817, 'fps': 25.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': 128, 'metadata': {'Metadata': '', 'handler_name': 'SoundHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 32.0, 'bitrate': 952, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [854, 480], 'video_bitrate': 817, 'video_fps': 25.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 128, 'video_duration': 32.0, 'video_n_frames': 800}\n",
      "/home2/sarthak395/miniconda3/envs/sony/lib/python3.10/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i /ssd_scratch/cvit/sarthak395/outputs/IyMdcXl4vag/raw_videos/IyMdcXl4vag_1.mp4 -loglevel error -f image2pipe -vf scale=854:480 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "(854, 480)\n"
     ]
    }
   ],
   "source": [
    "clip = VideoFileClip('/ssd_scratch/cvit/sarthak395/outputs/IyMdcXl4vag/raw_videos/IyMdcXl4vag_1.mp4')\n",
    "        \n",
    "# get image size of the first frame\n",
    "first_frame = clip.get_frame(0)\n",
    "img_size = (first_frame.shape[1] , first_frame.shape[0]) # (W , H)\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
